{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.layers.core import Activation,Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('labeledTrainData.tsv',header=0, delimiter=\"\\t\", quoting=3)\n",
    "test=pd.read_csv('testData.tsv',header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    review_text=BeautifulSoup(review,'html.parser').get_text()\n",
    "    review_text=re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    words=review_text.lower()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review']=train['review'].map(review_to_wordlist)\n",
    "test['review']=test['review'].map(review_to_wordlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xvalid,ytrain,yvalid=train_test_split(train.review.values,\n",
    "                                             train.sentiment.values,stratify=train.sentiment.values,random_state=42,\n",
    "                                            shuffle=True,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python36\\lib\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec.load('Word2Vector-300features_40minwords_10context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_word2vec = [sent2vec(x) for x in xtrain]\n",
    "xvalid_word2vec = [sent2vec(x) for x in xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "from sklearn import preprocessing\n",
    "scl=preprocessing.StandardScaler()\n",
    "xtrain_word2vec_scl=scl.fit_transform(xtrain_word2vec)\n",
    "xvalid_word2vec_scl=scl.fit_transform(xvalid_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To move further, i.e. with LSTMs we need to tokenize the text data\n",
    "from keras.preprocessing import sequence, text\n",
    "token=text.Tokenizer(num_words=None)\n",
    "max_len=80\n",
    "# fit_on_text(texts) 使用一系列文档来生成token词典，texts为list类，每个元素为一个文档。\n",
    "# texts_to_sequences(texts) 将多个文档转换为word下标的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "# texts_to_matrix(texts) 将多个文档转换为矩阵表示,shape为[len(texts),num_words]\n",
    "token.fit_on_texts(list(xtrain)+list(xvalid))\n",
    "xtrain_seq=token.texts_to_sequences(xtrain)\n",
    "xvalid_seq=token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad=sequence.pad_sequences(xtrain_seq,maxlen=max_len)\n",
    "xvalid_pad=sequence.pad_sequences(xvalid_seq,maxlen=max_len)\n",
    "\n",
    "word_index=token.word_index\n",
    "# word_index 一个dict，保存所有word对应的编号id，从1开始\n",
    "# word_counts 一个dict，保存每个word在所有文档中出现的次数\n",
    "# word_docs 一个dict，保存每个word出现的文档的数量\n",
    "# index_docs 一个dict，保存word的id出现的文档的数量\n",
    "index_word={v:k for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "EMBEDDING_SIZE=128\n",
    "HIDDEN_LAYER_SIZE=64\n",
    "model=Sequential()\n",
    "model.add(Embedding(len(word_index),EMBEDDING_SIZE,\n",
    "                   input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE,dropout=0.2,recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 82s - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 76s - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 76s - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 76s - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 74s - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b4ca978>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=64\n",
    "NUM_EPOCHS=10\n",
    "# Fit the model with early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "model.fit(xtrain_pad,ytrain,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,validation_data=(xvalid_pad,yvalid),callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
