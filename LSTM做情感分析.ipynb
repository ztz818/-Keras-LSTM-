{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation,Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk#分词\n",
    "import collections#用来统计词频\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们需要知道数据中有多少个不同的单词，每句话由多少个单词组成。 \n",
    "maxlen=0\n",
    "word_freqs=collections.Counter()#为可以进行哈希的对象计数\n",
    "num_recs=0#样本数\n",
    "with open ('train_data.txt','r+',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        label,sentence=line.strip().split('\\t')\n",
    "        words=nltk.word_tokenize(sentence.lower())\n",
    "        if len(words)>maxlen:\n",
    "            maxlen=len(words)\n",
    "        for word in words:\n",
    "            word_freqs[word]+=1\n",
    "        num_recs+=1\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:42\n",
      "nb_words:2330\n"
     ]
    }
   ],
   "source": [
    "print('max_len:{}'.format(maxlen))\n",
    "print('nb_words:{}'.format(len(word_freqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据不同单词的个数 (nb_words)，我们可以把词汇表的大小设为一个定值，并且对于不在词汇表里的单词，把它们用伪单词 UNK 代替。 根据句子的最大长度 (max_lens)，我们可以统一句子的长度，把短句用 0 填充。 \n",
    "#依前所述，我们把 VOCABULARY_SIZE 设为 2002。包含训练数据中按词频从大到小排序后的前 2000 个单词，外加一个伪单词 UNK 和填充单词 0。 最大句子长度 MAX_SENTENCE_LENGTH 设为40。 \n",
    "MAX_FEATURES=2000\n",
    "MAX_SENTENCE_LENGTH=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下来建立两个 lookup tables，分别是 word2index 和 index2word，用于单词和数字转换。 \n",
    "vocab_size=min(MAX_FEATURES,len(word_freqs))+2\n",
    "word2index={x[0]: i+2 for i,x in enumerate(word_freqs.most_common(MAX_FEATURES)) }\n",
    "word2index['PAD']=0\n",
    "word2index['UNK']=1\n",
    "index2word={v:k for k,v in word2index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面就是根据 lookup table 把句子转换成数字序列了，并把长度统一到 MAX_SENTENCE_LENGTH， 不够的填 0 ， 多出的截掉\n",
    "X=np.empty(num_recs,dtype=list)\n",
    "y=np.zeros(num_recs)\n",
    "i=0\n",
    "with open('train_data.txt','r+',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        label,sentence=line.strip().split('\\t')\n",
    "        words=nltk.word_tokenize(sentence.lower())\n",
    "        seqs=[]\n",
    "        for word in words:\n",
    "            if word in word2index:\n",
    "                seqs.append(word2index[word])\n",
    "            else:\n",
    "                seqs.append(word2index['UNK'])\n",
    "        X[i]=seqs       \n",
    "        \n",
    "        y[i]=int(label)\n",
    "    \n",
    "        i+=1\n",
    "X=sequence.pad_sequences(X,maxlen=MAX_SENTENCE_LENGTH)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据准备好后，就可以上模型了。这里损失函数用 binary_crossentropy， 优化方法用 adam。 至于 EMBEDDING_SIZE , HIDDEN_LAYER_SIZE , 以及训练时用到的BATCH_SIZE 和 NUM_EPOCHS 这些超参数，就凭经验多跑几次调优了。 \n",
    "EMBEDDING_SIZE=128\n",
    "HIDDEN_LAYER_SIZE=64\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "# Embedding层只能作为模型的第一层\n",
    "\n",
    "# 参数\n",
    "# input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1\n",
    "\n",
    "# output_dim：大于0的整数，代表全连接嵌入的维度\n",
    "\n",
    "# embeddings_initializer: 嵌入矩阵的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers\n",
    "\n",
    "# embeddings_regularizer: 嵌入矩阵的正则项，为Regularizer对象\n",
    "\n",
    "# embeddings_constraint: 嵌入矩阵的约束项，为Constraints对象\n",
    "\n",
    "# mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为|vocabulary| + 1。\n",
    "\n",
    "# input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE,dropout=0.2,recurrent_dropout=0.2))\n",
    "#recurrent_dropout：0~1之间的浮点数，控制循环状态的线性变换的神经元断开比例\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 8s - loss: 0.2354 - acc: 0.9033 - val_loss: 0.0663 - val_acc: 0.9781\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0246 - acc: 0.9931 - val_loss: 0.0555 - val_acc: 0.9795\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0453 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0461 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0545 - val_acc: 0.9873\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0546 - val_acc: 0.9908\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0487 - val_acc: 0.9901\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 6s - loss: 8.7800e-04 - acc: 0.9998 - val_loss: 0.0586 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0721 - val_acc: 0.9795\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 6s - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0484 - val_acc: 0.9908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x8eead30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=32\n",
    "NUM_EPOCHS=10\n",
    "model.fit(Xtrain,ytrain,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,\n",
    "         validation_data=(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================] - 1s     \n",
      "\n",
      "Test score : 0.048,accuracy:0.991\n",
      "预测  真实    句子\n",
      " 1      1     i am going to start reading the harry potter series again because that is one awesome story .\n",
      " 1      1     the last stand and mission impossible 3 both were awesome movies .\n",
      " 1      1     mission impossible 3 was awesome..\n",
      " 0      0     i think i hate harry potter because it outshines much better reading material out there and the movies are just plain stupid to begin with .\n",
      " 1      1     sunday before that we went and saw mission impossible 3 so that was awesome .\n"
     ]
    }
   ],
   "source": [
    "#我们用已经训练好的 LSTM 去预测已经划分好的测试集的数据，查看其效果。选了 5 个句子的预测结果，并打印出了原句。\n",
    "\n",
    "score,acc=model.evaluate(Xtest,ytest,batch_size=BATCH_SIZE)\n",
    "print(\"\\nTest score : %.3f,accuracy:%.3f\"%(score,acc))\n",
    "\n",
    "print('{}  {}    {}'.format('预测','真实','句子'))\n",
    "for i in range(5):\n",
    "    idx=np.random.randint(len(Xtest))\n",
    "    xtest=Xtest[idx].reshape(1,40)\n",
    "    \n",
    "    ylabel=ytest[idx]\n",
    "    \n",
    "    ypred=model.predict(xtest)[0][0]\n",
    "    \n",
    "    sent=' '.join([index2word[x] for x in xtest[0] if x!=0])\n",
    "    print(' {}      {}     {}'.format(int(round(ypred)), int(ylabel), sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "积极   I love reading.\n",
      "消极   You are so boring.\n"
     ]
    }
   ],
   "source": [
    "INPUT_SENTENCES=['I love reading.','You are so boring.']\n",
    "XX=np.empty(len(INPUT_SENTENCES),dtype=list)\n",
    "i=0\n",
    "for sentence in INPUT_SENTENCES:\n",
    "    words=nltk.word_tokenize(sentence.lower())\n",
    "    seq=[]\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seq.append(word2index[word])\n",
    "        else:\n",
    "            seq.append(word2index['UNK'])\n",
    "    XX[i] = seq\n",
    "    i+=1\n",
    "XX = sequence.pad_sequences(XX, maxlen=MAX_SENTENCE_LENGTH)\n",
    "labels = [int(round(x[0])) for x in model.predict(XX) ]\n",
    "label2word = {1:'积极', 0:'消极'}\n",
    "for i in range(len(INPUT_SENTENCES)):\n",
    "    print('{}   {}'.format(label2word[labels[i]], INPUT_SENTENCES[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
